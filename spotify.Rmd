---
title: "Group Project: Spotify" 
author: "Chu Li (1092477), Denis Lebedev (4826973)，Tinghui Xu (1715119), Eleni Spyrou (4515919)"
date: 2025
output:
    prettydoc::html_pretty:
      theme: cayman
      highlight: github
      toc: true
      number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  comment = NA
)

library(readr)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(ISLR)
library(glmnet)
library(tidyverse)
library(caret)
library(corrplot)
```


```{r}
set.seed(123)
Spotify <- read.csv("Spotify-2000.csv")
colnames(Spotify)
```


```{r data-cleaning, warning=FALSE, message=FALSE}
str(Spotify) # check the structure of the dataset
colSums(is.na(Spotify)) # check for missing values in the dataset.
# The dataset has no missing values.

# We then clean the dataset by renaming the columns for clarity and converting the relevant columns to numeric.
# Check which columns are not clean numerics
glimpse(Spotify)

# Cleaned version (only mutate numeric columns that are truly numeric)
spotify_data <- Spotify %>%
  rename(
    BPM = `Beats.Per.Minute..BPM.`,
    Loudness = `Loudness..dB.`,
    Genre = `Top.Genre`,
    Length = `Length..Duration.`
  ) %>%
  mutate(
    BPM = as.numeric(BPM),
    Energy = as.numeric(Energy),
    Danceability = as.numeric(Danceability),
    Loudness = as.numeric(Loudness),
    Valence = as.numeric(Valence),
    Length = as.numeric(Length),
    Acousticness = as.numeric(Acousticness),
    Speechiness = as.numeric(Speechiness),
    Liveness = as.numeric(Liveness),
    Popularity = as.numeric(Popularity),
    Year = as.numeric(Year)
  )
```

# (1) A green histogram showing the distribution of the number of tracks in different popularity ranges.
X-axis: represents the “Popularity Score”, which ranges from 0 to 100 on a scale of 25 (0, 25, 50, 75, 100).
Y-axis: represents the “Count of Tracks”, ranging from 0 to 250 on a scale of 100.

```{r plot-popularity-histogram, fig.align='center', fig.height=5, fig.width = 7}
# Plot histogram of popularity
ggplot(spotify_data, aes(x = Popularity)) +
  geom_histogram(binwidth = 5, fill = "darkgreen", color = "white", alpha = 0.8) +
  labs(
    title = "Distribution of Track Popularity",
    x = "Popularity Score",
    y = "Count of Tracks"
  ) +
  theme_minimal()

```

# Key observations:
  (a) The distribution of the popularity scores is right skewed (positively skewed), i.e., the number of tracks with low popularity (0-25) is low, and the number of tracks with high popularity (75-100) is also low, with the majority of tracks clustered in the 50-75 range. This indicates a slight bimodal character (bimodal) to the distribution, with the main peak in the 60-75 range and the secondary peak in the 50-60 range.
  (b) This suggests that most tracks have a concentration of moderate popularity, with fewer very high or very low popularity, reflecting the trend of concentration in music popularity.



# (2) We decided to plot a line graph to represent the average annual track popularity.
X-axis: represents “Release Year”, which spans from 1960 to 2020, and is scaled in 10-year increments (1960, 1980, 2000, 2020).
Y-axis: represents “Average Popularity”, which spans from 50 to 70 and is scaled in units of 10.
We use a blue line to represent the average popularity of the tracks for each year, and a light blue shaded area around the line to represent the confidence interval, which shows the variability of the data. There is also a blue linear trend line running through the entire chart, reflecting the overall trend.

```{r plot-popularity-year, warning=FALSE, message=FALSE, fig.align='center', fig.height=5, fig.width = 7}
# Plot average popularity by year with linear model
spotify_data %>%
  group_by(Year) %>%
  summarise(Avg_Popularity = mean(Popularity, na.rm = TRUE)) %>%
  ggplot(aes(x = Year, y = Avg_Popularity)) +
  geom_line(color = "steelblue", size = 1) +
  geom_smooth(method = "lm", color = "blue", fill = "lightblue", se = TRUE) + 
  labs(
    title = "Average Track Popularity Over Years",
    x = "Release Year",
    y = "Average Popularity"
  ) +
  theme_minimal()

```
# Key observations:
  (a) From 1960 to 2020, average repertoire prevalence shows a general downward trend, dropping to a low around 2000, near 50. after 2000, the trend begins to stabilize and shows slight fluctuations and signs of increase between 2010 and 2020, with prevalence rising slightly back above 50 in 2020.
  (b) Volatility: Prevalence fluctuates significantly between years. For example, there was a sharp peak in the early 1960s followed by a rapid decline; flatter fluctuations in the 1980s and 1990s; and several small increases and decreases in the late 2000s and early 2010s.
  (c) Confidence intervals: The light blue shaded areas indicate confidence intervals for the mean prevalence, reflecting the variability and statistical reliability of the data. In the 1960s and 2020s, the confidence intervals are narrower, suggesting that the data are less variable in these years and that the data may have larger sample sizes or a more concentrated distribution of prevalence. In contrast, in the 1980s through the 2000s, the confidence intervals are slightly wider, indicating that the data are more variable, possibly due to increased differences in the popularity of different types of music.
  (d) Trend line: The blue linear trend line shows that the long-term trend is downward, with a negative slope. This trend line was fitted by a linear regression model and shows that despite short-term fluctuations, overall average prevalence has declined by about 15-20 points over the past 60 years (from 67 to about 50).


# (3) We decide to draw a heatmap to visualize the correlation between the numeric variables in the dataset. 
  (a) Heatmaps use colors and numbers to indicate the linear correlation between variables, with values ranging from -1 to 1.
  (b) The colors range from red (negative correlation) to blue (positive correlation), and the darker the color, the stronger the correlation.
  (c) The area above the diagonal is blank (only the lower triangle is displayed), which improves simplicity.
  (d) The heatmap will help us identify any strong correlations between features, which can be useful for feature selection in predictive modeling.

```{r plot-correlation-heatmap, fig.align='center', fig.height=5, fig.width = 7}
# Select numeric variables
num_data <- spotify_data %>%
  select(where(is.numeric))

# Compute correlation matrix
cor_matrix <- cor(num_data, use = "complete.obs")

# Plot heatmap
corrplot(cor_matrix, method = "color", type = "lower", 
         tl.col = "black", tl.srt = 45, number.cex = 0.7,
         addCoef.col = "black", diag = FALSE)
```
# Key observations:
  (a) Energy and Loudness have the highest correlation (0.74): music with high energy tends to be louder.
  (b) Acousticness and Energy have the strongest negative correlation (-0.67): music with high acoustics tends to have low energy (softer).
  (c) Popularity has weak correlations with other variables (the absolute value is basically < 0.2), indicating that popularity is affected by more complex factors and is not determined by these audio features alone.


# (4) Linear Model: deviding the dataset into train, valid and test
  (a)Firstly we got read of all NA values by using the `na.omit` function
  (b)Then used the `createDataPartition` function to create an index for the distribytion of the variable, Popularity`.
  (c)The division of the original is 50%, 30%, and 20% for the train, valid and test respectively.
  (d)To split the division we made two new variables:
                                                 -   `df_train`, which coresponds to 50% of the variable,
                                                 -   `remaining`, which is takeng the remaining 50%, 
                                                 -   `df_valid`, which uses 60% of the remaining, and lastly
                                                 -   `df_test`, which takes whatever is left from `df_valid`
  (e)Then we graph each one by adding `Set` to each row identifying which dataset it belongs to.
  (f)We combine the tree sets into one data set, using the function `bind_rows`, which is used to combine df vertivally.
  (g)Lastly we visualize the combined data frame using `ggplot2` whith a density graph.


```{r, fig.align='center', fig.height=5, fig.width = 7}
spotify_data <- na.omit(spotify_data)

#creating train, test and valid datasets
train_index <- createDataPartition(spotify_data$Popularity, p = 0.5, list = FALSE)
df_train <- spotify_data[train_index, ]

remaining <- spotify_data[-train_index, ]
valid_index <- createDataPartition(remaining$Popularity, p = 0.6, list = FALSE)
df_valid <- remaining[valid_index, ]

df_test <- remaining[-valid_index, ]

#graph to make sure of distribution
df_train$Set <- "Train"
df_valid$Set <- "Validation"
df_test$Set <- "Test"

combined <- bind_rows(df_train, df_valid, df_test)

ggplot(combined, aes(x = Popularity, fill = Set)) +
  geom_density(alpha = 0.4) +
  theme_minimal()
```

# Key Observations:
  (a)X-axis:Popularity
  (b)Y-axis:Density
  (c)Overall the distribution of the tree sets is overlapping very closely.
  (d)This indicates that the model is idealy ploted and has no major distributional bias introduced during the split.


# (5) Linear Model: choosing the best model based on MSE
  (a)Firslty we generated some formula in R and sourced it to our Rmrd. 
  (b)We define the lm_mse function with `lm_mse <- function(formula, train_data, valid_data)`, which means that we will train the model using the training data, but 
  predict it using the valid data. Eventually, the formula we built will give us the mean of squared errors, which is the mean of the square of the difference between the 
  true and predicted Y values. 
  (c)When it comes to identifying predictors we deselected some from the data so we are left only with the ones that interest us. 
  (d)To asses the best model, we want the model with the lowest mean square errors. 
  (e)For this step we initialy classed the object `top_formulas` as a list, and the object `top_mses` as numeric. 
  (f)In order to move on with finding the best model we are using loops, where we evaluate all the models by numbers of predictors. 
  (g)We generated a formula, where a formula for every model is generated with regard to that model's predictors for popularity. 
  (h)Then, we state that mses can be repeated from 0 until the amount of formulas we have. 
  (i)Following that, the mses of a specific formula can be calculated through using the lm_mses formula from part 4. 
  (j)Eventualy we remade the two objects, top_formula and top_mses, where the outcome of top_formula coresponds to the formula with the lowest mses and the output of 
  top_mses will be the actual best mses. 
  (k)For the final step, we want the output of our code to be writen in a sentence, so we formulate a senterce, where we replace the word "MSE" with the number 
  coresponding to the `top_mses` rounded to 2 decimal places. 
  (l)Other than the sentencce we also add the linear formula of the model, consisting of the independent variable and its predictors.

```{r}
#some preparations: functions for formula generation and MSE calculation
source("generate_formulas.R")

lm_mse <- function(formula, train_data, valid_data) {
  y_name <- as.character(formula)[2]
  y_true <- valid_data[[y_name]]
  
  lm_fit <- lm(formula, train_data)
  y_pred <- predict(lm_fit, newdata = valid_data)
  
  mean((y_true - y_pred)^2)
}

# identifying predictors
df_pred <- spotify_data %>% select(-c("Title", "Artist", "Genre", "Popularity", "Index"))
predictors <- colnames(df_pred)

# assessing the best model
top_formulas <- list()
top_mses <- numeric()

for (p in 1:length(predictors)) {
  forms <- generate_formulas(p = p, x_vars = predictors, y_var = "Popularity")
  mses <- rep(0, length(forms))

  for (i in 1:length(forms)) {
    mses[i] <- lm_mse(as.formula(forms[i]), df_train, df_valid)
  }

  top_formulas[[p]] <- forms[which.min(mses)]
  top_mses[p] <- mses[which.min(mses)]
}

#print("Top formulas for each number of predictors")
#top_formulas

print(paste("Best overall formula based on MSE (MSE =", round(min(top_mses), digits = 2), ")"))
best_model <- top_formulas[[which.min(top_mses)]]
best_model
```

# Key Observations:
  (a)What we can clearly see is that the best model is the one with more than 2 predictors.
  (b)This outlines that the more the predictors, the smaller the mean square error, the less the bias, and the better the model.

# (6) Linear model: calculate test MSE
  (a)We start of by setting a function for mse, which is the mean of the difference between the actual and predicted values squared.
  (b)Then we combine the train data and valid data vertically into a new variable called `train_valid`
  (c)Following that we make an element `best_model`, featuring the linear model of the the best model function from the last part, but now trained on the training and validation sets.
  (d)We then use the test set to predict the best model.
  (e)Lastly we test the MSE, with a model trained on the train and valid df, but predicted onn the test df.
  (f)We formulate our output into a phrase indicating the value of the MSE to 2 decimal places.

```{r}
#calculate test MSE
mse <- function(actual, predicted) {
  mean((actual - predicted)^2)
}

train_valid <- bind_rows(df_train, df_valid)
best_model <- lm(best_model, data = train_valid)

pred_test <- predict(best_model, newdata = df_test)
test_mse <- mse(df_test$Popularity, pred_test)

print(paste("Test MSE =", round(test_mse, digits = 2)))
```

# Key Observations:
  (a)Our MSE when using this formula differs. In this part we used all the available information that we had and the return was better.
  (b)Now instea of 191.31, the MSE is 174.39, which is lower and improved.
  (c)This shows that when using also the test data other than the valid and train data, the formula is operating better.


# (7) Showing the results graphically
  (a)In order to graph our results we started off, by making a data frame of the actual and predicted results called `results_df`
  (b)We then used `ggplot2` again to visualize the data in a scater plot, and decided on using `alpha = 0.6` for less overlapping.
  X-axs: Predicted values
  Y-axis: Actual values
  (c)Furthermore, we added a diagonal line, showing when predictd=actual, using the function `geom_abline`.
  (d)We added colour to the line, title to the graph, labels to the axis, and a theme.
  (e)To finidh it up we made some justifications to the title in terms of position.

```{r, fig.align='center', fig.height=5, fig.width = 7}
results_df <- data.frame(
  Actual = df_test$Popularity,
  Predicted = pred_test
)

ggplot(results_df, aes(x = Predicted, y = Actual)) +
  geom_point(alpha = 0.6, size = 2) +
  geom_abline(slope = 1, intercept = 0, color = "red", size = 1) +
  labs(title = "Predicted vs Actual Popularity",
       x = "Predicted Popularity",
       y = "Actual Popularity") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```
# Key Observations:
  (a)The points are widely spread among the values showing not very strong accuracy.
  (b)There is no much overlapping though, and there are a fee point on the line indicating equivalence of predicted and actual.
  (c)Most predicted values cluster at the values 50-65, eventhough actual values do at 25-80.
  (d)There is some bias between the two and a tedency to regress towards the mean.


# student’s contribution:

Chu Li(1092477): step 1&2
Denis Lebedev (4826973): step 3 (code) + some cosmetic features for all sections
Tinghui Xu (1715119):step 1&2 previous examples and revises 
Eleni Spyrou (4515919): step 3&4 

# AI tools
